{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Objectives\n",
    "\n",
    "This lab aims to show how to detect malwares with decision trees, as well as several techniques that can be used to tackle class imbalance issue. You will learn how to\n",
    "\n",
    "- Train a decision tree classifier for malware detection.\n",
    "- upsampling and downsampling to tackle the challenge of class imbalance.\n",
    "\n",
    "Please make sure this notebook and the following dataset files are located in the same folder:\n",
    "\n",
    "- dataset \"MalwareArtifacts.csv\" for part 1\n",
    "- datasets \"training_data.npz\", \"training_labels.npy\", \"test_data.npz\", \"test_labels.npy\" for part 2\n",
    "\n",
    "Please try out the following cells and run the python code in your notebook. \n",
    "\n",
    "***\n",
    "***This is not an assignment and you do not need to submit it***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Detecting malwares with decision trees\n",
    "\n",
    "In this part, we will use the \"AddressOfEntryPoint\" and \"DllCharacteristics\" fields that are stored in \"MalwareArtifacts.csv\" as potentially distinctive features for detecting malware. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "malware_dataset = pd.read_csv('MalwareArtifacts.csv', delimiter=',')\n",
    "# Extact artifacts samples fields \"AddressOfEntryPoint\" and \"DllCharacteristics\"\n",
    "samples = malware_dataset.iloc[:, [0, 4]].values\n",
    "targets = malware_dataset.iloc[:, 8].values\n",
    "\n",
    "# split the dataset into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_samples, testing_samples, training_targets, testing_targets = train_test_split(\n",
    "         samples, targets, test_size=0.2, random_state=0)\n",
    "\n",
    "# train a decision tree classifier then make predictions.\n",
    "from sklearn import tree\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_classifier.fit(training_samples, training_targets)\n",
    "predictions = tree_classifier.predict(testing_samples)\n",
    "\n",
    "accuracy = 100.0 * accuracy_score(testing_targets, predictions)\n",
    "print (\"Decision Tree accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results, the accuracy of the forecasts made by selecting the AddressOfEntryPoint and DllCharacteristics fields proves particularly effective, being higher than 96%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tackling class imbalance\n",
    "\n",
    "In part 2, we will look at some commonly used techniques to tackle the challenge of class imbalance. Often in applying machine learning to cybersecurity, we are faced with highly imbalanced datasets. For instance, it may be much easier to access a large collection of benign samples than it is to collect malicious samples. Conversely, you may be working at an enterprise that, for legal reasons, is prohibited from saving benign samples. In either case, your dataset will be highly skewed toward one class. As a consequence, naive machine learning aimed at maximizing accuracy will result in a classifier that predicts almost all samples as coming from the overrepresented class. In the following steps, we will demonstrate several methods for dealing with imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Begin by loading the training and testing data, and importing some libraries we will be using to score performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import scipy.sparse\n",
    "import collections\n",
    "\n",
    "X_train = scipy.sparse.load_npz(\"training_data.npz\")\n",
    "y_train = np.load(\"training_labels.npy\")\n",
    "X_test = scipy.sparse.load_npz(\"test_data.npz\")\n",
    "y_test = np.load(\"test_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train and test a simple Decision Tree classifier:\n",
    "\n",
    "(balanced_accuracy_score: The balanced accuracy in classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "# Count the number of instances of predicted label in dt_pred \n",
    "print(collections.Counter(dt_pred)) \n",
    "# print the balanced accuracy score\n",
    "print(balanced_accuracy_score(y_test, dt_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test several techniques to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Upsampling the minor class**: We extract all test samples from class 0 and class 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "X_train_np = X_train.toarray()\n",
    "class_0_indices = [i for i, x in enumerate(y_train == 0) if x]\n",
    "class_1_indices = [i for i, x in enumerate(y_train == 1) if x]\n",
    "size_class_0 = sum(y_train == 0)\n",
    "X_train_class_0 = X_train_np[class_0_indices, :]\n",
    "y_train_class_0 = [0] * size_class_0\n",
    "X_train_class_1 = X_train_np[class_1_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We upsample the elements of class 1 with replacements until the number of samples of class 1 and class 0 are equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_class_1_resampled = resample(\n",
    "    X_train_class_1, replace=True, n_samples=size_class_0\n",
    ")\n",
    "y_train_class_1_resampled = [1] * size_class_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We combine the newly upsampled samples into a single training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled = np.concatenate([X_train_class_0, X_train_class_1_resampled])\n",
    "y_train_resampled = y_train_class_0 + y_train_class_1_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We train and test a classifier on our upsampled training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "X_train_resampled = sparse.csr_matrix(X_train_resampled)\n",
    "dt_resampled = tree.DecisionTreeClassifier(random_state=1)\n",
    "dt_resampled.fit(X_train_resampled, y_train_resampled)\n",
    "dt_resampled_pred = dt_resampled.predict(X_test)\n",
    "print(collections.Counter(dt_resampled_pred))\n",
    "print(balanced_accuracy_score(y_test, dt_resampled_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Downsampling the major class**: We perform similar steps to the previous upsampling, except this time we down-sample the major class until it is of the same size as the minor class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.toarray()\n",
    "class_0_indices = [i for i, x in enumerate(y_train == 0) if x]\n",
    "class_1_indices = [i for i, x in enumerate(y_train == 1) if x]\n",
    "size_class_1 = sum(y_train == 1)\n",
    "X_train_class_1 = X_train_np[class_1_indices, :]\n",
    "y_train_class_1 = [1] * size_class_1\n",
    "X_train_class_0 = X_train_np[class_0_indices, :]\n",
    "X_train_class_0_downsampled = resample(\n",
    "    X_train_class_0, replace=False, n_samples=size_class_1\n",
    ")\n",
    "y_train_class_0_downsampled = [0] * size_class_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. We create a new training set from the downsampled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_downsampled = np.concatenate([X_train_class_1, X_train_class_0_downsampled])\n",
    "y_train_downsampled = y_train_class_1 + y_train_class_0_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. We train a classifier on this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_downsampled = sparse.csr_matrix(X_train_downsampled)\n",
    "dt_downsampled = tree.DecisionTreeClassifier(random_state=1)\n",
    "dt_downsampled.fit(X_train_downsampled, y_train_downsampled)\n",
    "dt_downsampled_pred = dt_downsampled.predict(X_test)\n",
    "print(collections.Counter(dt_downsampled_pred))\n",
    "print(balanced_accuracy_score(y_test, dt_downsampled_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summirize part 2, we start by loading in a predefined dataset (step 1). Our next step is to train a basic Decision Tree model on our data (step 2). To measure performance, we utilize the balanced accuracy score, a measure that is often used in classification problems with imbalanced datasets. By definition, balanced accuracy is the average of recall obtained on each class. The best value is 1, whereas the worst value is 0.\n",
    "\n",
    "In the following steps, we employ different techniques to tackle the class imbalance. In steps 3 to 6, we utilize upsampling to tackle class imbalance. This is the process of randomly duplicating observations from the minority class in order to reinforce\n",
    "the minority class's signal. There are several methods for doing so, but the most common way is to simply resample with replacements as we have done. In steps 7 to 9, we down-sample our major class. This simply means that we don't use all of the samples we have, but just enough so that we balance our classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Hands On AI for Cybersecurity - Detecting malwares with decision trees\n",
    "- Machine Learning for Cybersecurity Cookbook -  Machine Learning-Based Malware Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***end***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
