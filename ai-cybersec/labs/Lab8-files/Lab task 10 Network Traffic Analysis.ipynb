{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Objectives\n",
    "\n",
    "This lab shows a case study of classifing network attacks that was discussed in the lecture. Do not worry if you cannot fully understand the code. The focus is to give you a hands-on practice and show how building high-performing machine learning systems is a process filled with exploration and experimentation.\n",
    "\n",
    "Please make sure this notebook and the folder 'datasets' are located in the same directory.\n",
    "\n",
    "Please try out the following cells and run the python code in your notebook. \n",
    "\n",
    "***\n",
    "***This is not an assignment and you do not need to submit it***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and processing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory containing all of the relevant data files\n",
    "dataset_root = 'datasets/nsl-kdd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the data that we are using:\n",
    "train_file = os.path.join(dataset_root, 'KDDTrain+.txt')\n",
    "test_file = os.path.join(dataset_root, 'KDDTest+.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header_names is a list of feature names in the same order as the data\n",
    "header_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type', 'success_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiating between nominal, binary, and numeric features\n",
    "\n",
    "# root_shell is marked as a continuous feature in the kddcup.names \n",
    "# file, but it is supposed to be a binary feature according to the \n",
    "# dataset documentation\n",
    "\n",
    "col_names = np.array(header_names)\n",
    "\n",
    "nominal_idx = [1, 2, 3]\n",
    "binary_idx = [6, 11, 13, 14, 20, 21]\n",
    "numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx))\n",
    "\n",
    "nominal_cols = col_names[nominal_idx].tolist()\n",
    "binary_cols = col_names[binary_idx].tolist()\n",
    "numeric_cols = col_names[numeric_idx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_attack_types.txt maps each of the 22 different attacks to 1 of 4 categories\n",
    "# file obtained from http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types\n",
    "\n",
    "# The directory containing all of the relevant data files\n",
    "category = defaultdict(list)\n",
    "category['benign'].append('normal')\n",
    "\n",
    "with open('datasets/training_attack_types.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        attack, cat = line.strip().split(' ')\n",
    "        category[cat].append(attack)\n",
    "\n",
    "attack_mapping = dict((v,k) for k in category for v in category[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and analyzing train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data\n",
    "train_df = pd.read_csv(train_file, names=header_names)\n",
    "train_df['attack_category'] = train_df['attack_type'] \\\n",
    "                                .map(lambda x: attack_mapping[x])\n",
    "train_df.drop(['success_pred'], axis=1, inplace=True)\n",
    "\n",
    "# Read test data\n",
    "test_df = pd.read_csv(test_file, names=header_names)\n",
    "test_df['attack_category'] = test_df['attack_type'] \\\n",
    "                                .map(lambda x: attack_mapping[x])\n",
    "test_df.drop(['success_pred'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at the attack_type and attack_category distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attack_types = train_df['attack_type'].value_counts()\n",
    "train_attack_cats = train_df['attack_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attack_types.plot(kind='barh', figsize=(20,10), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attack_cats.plot(kind='barh', figsize=(20,10), fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the binary features\n",
    "# By definition, all of these features should have a min of 0.0 and a max of 1.0\n",
    "\n",
    "train_df[binary_cols].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a minute... the su_attempted column has a max value of 2.0?\n",
    "\n",
    "train_df.groupby(['su_attempted']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix this discrepancy and assume that su_attempted=2 -> su_attempted=0\n",
    "\n",
    "train_df['su_attempted'].replace(2, 0, inplace=True)\n",
    "test_df['su_attempted'].replace(2, 0, inplace=True)\n",
    "train_df.groupby(['su_attempted']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we notice that the num_outbound_cmds column only takes on one value!\n",
    "\n",
    "train_df.groupby(['num_outbound_cmds']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, that's not a very useful feature - let's drop it from the dataset\n",
    "\n",
    "train_df.drop('num_outbound_cmds', axis = 1, inplace=True)\n",
    "test_df.drop('num_outbound_cmds', axis = 1, inplace=True)\n",
    "numeric_cols.remove('num_outbound_cmds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "Let’s begin by splitting the test and training DataFrames into data and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_df['attack_category']\n",
    "train_x_raw = train_df.drop(['attack_category','attack_type'], axis=1)\n",
    "test_Y = test_df['attack_category']\n",
    "test_x_raw = test_df.drop(['attack_category','attack_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further data processing which you can ignore.\n",
    "combined_df_raw = pd.concat([train_x_raw, test_x_raw])\n",
    "combined_df = pd.get_dummies(combined_df_raw, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "train_x = combined_df[:len(train_x_raw)]\n",
    "test_x = combined_df[len(train_x_raw):]\n",
    "\n",
    "# Store dummy variable feature names\n",
    "dummy_variables = list(set(train_x)-set(combined_df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example statistics for the 'duration' feature before scaling\n",
    "train_x['duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with StandardScaler on the single 'duration' feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "durations = train_x['duration'].values.reshape(-1, 1)\n",
    "standard_scaler = StandardScaler().fit(durations)\n",
    "scaled_durations = standard_scaler.transform(durations)\n",
    "pd.Series(scaled_durations.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with MinMaxScaler on the single 'duration' feature\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler().fit(durations)\n",
    "min_max_scaled_durations = min_max_scaler.transform(durations)\n",
    "pd.Series(min_max_scaled_durations.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's proceed with StandardScaler- Apply to all the numeric columns\n",
    "\n",
    "standard_scaler = StandardScaler().fit(train_x[numeric_cols])\n",
    "\n",
    "train_x[numeric_cols] = \\\n",
    "    standard_scaler.transform(train_x[numeric_cols])\n",
    "\n",
    "test_x[numeric_cols] = \\\n",
    "    standard_scaler.transform(test_x[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by training a decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_bin = train_Y.apply(lambda x: 0 if x is 'benign' else 1)\n",
    "test_Y_bin = test_Y.apply(lambda x: 0 if x is 'benign' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-class classification version\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=17)\n",
    "classifier.fit(train_x, train_Y)\n",
    "\n",
    "pred_y = classifier.predict(test_x)\n",
    "\n",
    "results = confusion_matrix(test_Y, pred_y)\n",
    "error = zero_one_loss(test_Y, pred_y)\n",
    "\n",
    "print(results)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a KNN classifier. The training process may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=1, n_jobs=-1)\n",
    "classifier.fit(train_x, train_Y)\n",
    "\n",
    "pred_y = classifier.predict(test_x)\n",
    "\n",
    "results = confusion_matrix(test_Y, pred_y)\n",
    "error = zero_one_loss(test_Y, pred_y)\n",
    "\n",
    "print(results)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the results for a linear support vector classifier: (The training process may take some time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(train_x, train_Y)\n",
    "\n",
    "pred_y = classifier.predict(test_x)\n",
    "\n",
    "results = confusion_matrix(test_Y, pred_y)\n",
    "error = zero_one_loss(test_Y, pred_y)\n",
    "\n",
    "print(results)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rates are in the same ballpark, and the resulting confusion matrices show very similar characteristics to what we saw with the decision tree classifier results. At this point, it should be clear that continuing to experiment with classification algorithms might not be productive. Let's attempt unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's visualize the dataset (only numeric cols)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use PCA to reduce dimensionality so we can visualize the dataset on a 2d plot\n",
    "pca = PCA(n_components=2)\n",
    "train_x_pca_cont = pca.fit_transform(train_x[numeric_cols])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "colors = ['navy', 'turquoise', 'darkorange', 'red', 'purple']\n",
    "\n",
    "for color, cat in zip(colors, category.keys()):\n",
    "    plt.scatter(train_x_pca_cont[train_Y==cat, 0], train_x_pca_cont[train_Y==cat, 1],\n",
    "                color=color, alpha=.8, lw=2, label=cat)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-means (k=5, only using numeric cols) + PCA + plot\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit a k-means clustering estimator model\n",
    "kmeans = KMeans(n_clusters=5, random_state=17).fit(train_x[numeric_cols])\n",
    "\n",
    "# Retrieve the labels assigned to each training sample\n",
    "kmeans_y = kmeans.labels_\n",
    "\n",
    "# Plot in 2d with train_x_pca_cont\n",
    "plt.figure(figsize=(15,10))\n",
    "colors = ['navy', 'turquoise', 'darkorange', 'red', 'purple']\n",
    "\n",
    "for color, cat in zip(colors, range(5)):\n",
    "    plt.scatter(train_x_pca_cont[kmeans_y==cat, 0],\n",
    "                train_x_pca_cont[kmeans_y==cat, 1],\n",
    "                color=color, alpha=.8, lw=2, label=cat)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import completeness_score,\\\n",
    "    homogeneity_score, v_measure_score\n",
    "\n",
    "print('Completeness: {}'.format(completeness_score(test_Y, pred_y)))\n",
    "print('Homogeneity: {}'.format(homogeneity_score(test_Y, pred_y)))\n",
    "print('V-measure: {}'.format(v_measure_score(test_Y, pred_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V-measure score is not so good. One of the reasons is that k-means does not work well on nonspherical distributions. From the figure, it is difficult to argue that any of the classes have a spherical distribution. No wonder k-means does not work well here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Having worked through the exploration task, it should be clear that machine learning—specifically, data correlation and classification—is about more than just choosing the right classifier and knowing about algorithms. Spending time to explore and understand the data is key, and can help save you precious time in getting to a desired solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "- Network traffic analysis, Machine Learning and Security: Protecting Systems with Data and Algorithms. The full version of the code can be found at: https://github.com/oreilly-mlsec/book-resources/tree/master/chapter5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***end***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
