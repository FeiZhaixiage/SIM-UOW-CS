{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0109cc6f-c0a4-4081-8a5a-c3ed3195e27c",
   "metadata": {},
   "source": [
    "# Lab Objectives\n",
    "\n",
    "This lab aims to introduce the notion of perceptron, which is a type of linear classifier.\n",
    "\n",
    "- Definition and model representation\n",
    "- The role of bias\n",
    "- Perceptron for the logical NOT, AND, OR and XOR function\n",
    "\n",
    "Please try out the following cells and run the python code in your notebook. \n",
    "\n",
    "***\n",
    "***This is not an assignment and you do not need to submit it***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a152380-4076-4bf9-9652-07382ab811e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What is a Perceptron?\n",
    "A Perceptron is an algorithm used for supervised learning of binary classifiers. Binary classifiers decide whether an input, usually represented by a series of vectors, belongs to a specific class. Perceptron is an algorithm for learning a binary classifier called a step function: a function that maps its input $\\mathbf{x}$ (a real-valued vector) to an output value $f(\\mathbf{x})$ (a single binary value):\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x}) = \n",
    "        \\begin{cases}\n",
    "             1 & \\text{if $\\mathbf{w} \\cdot \\mathbf{x} + b \\geq 0$,}\\\\\n",
    "             -1 & \\text{otherwise}\\\\\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathbf {w}$  is a vector of real-valued weights, ${\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} }$ is the dot product ${\\displaystyle \\sum _{i=1}^{n}w_{i}x_{i}}$, $n$ is the number of inputs to the perceptron, and $b$ is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value.\n",
    "\n",
    "In short, a perceptron is a single-layer neural network with the activation of a step function. Perceptron is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1a9b2-5195-488b-9958-78a380b869c1",
   "metadata": {},
   "source": [
    "## How does a Perceptron work?\n",
    "\n",
    "The process begins by taking all the input values and multiplying them by their weights. Then, all of these multiplied values are added together to create the weighted sum. The weighted sum is then applied to the step function, producing the perceptron's output. It is important to note that the weight of an input is indicative of the strength of a node.\n",
    "\n",
    "Two sets of points (or classes) are called linearly separable, if at least one straight line in the plane exists so that all the points of one class are on one side of the line and all the points of the other class are on the other side.\n",
    "\n",
    "More formally: If two data clusters (classes) can be separated by a decision boundary in the form of a linear equation\n",
    "\n",
    "$$\\sum_{i=1}^n x_i \\cdot w_i + b = 0$$\n",
    "\n",
    "they are called linearly separable.\n",
    "\n",
    "Otherwise, i.e. if such a decision boundary does not exist, the two classes are called linearly inseparable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626faeec-d25b-4a7f-ae1c-971668cb2785",
   "metadata": {},
   "source": [
    "## Why do we need bias?\n",
    "\n",
    "Put a simpley way, a bias value allows you to shift the activation function curve up or down.\n",
    "\n",
    "Assume the two classes we want to classify in our example look like this: where dots of the same color belong to the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31aa50-07f1-4680-9e45-657a65d36f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"r\")\n",
    "ax.scatter(1, 0, color=\"r\")\n",
    "ax.scatter(1, 1, color=\"g\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "m = -1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3e16b-3e25-490d-a0f2-ae4ae37d318b",
   "metadata": {},
   "source": [
    "Suppose there is a perceptron model $y = m \\cdot x$ that is used to classify the two classess (red or green).\n",
    "We found out that such a perceptron is only capable of creating straight lines going through the origin $(0,0)$. So dividing lines like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2640b-5e8e-4e55-8c99-63a7c4bedff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "m = -1\n",
    "for m in np.arange(0, 6, 0.1):\n",
    "    ax.plot(X, m * X )\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"r\")\n",
    "ax.scatter(1, 0, color=\"r\")\n",
    "ax.scatter(1, 1, color=\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a893b-bc8b-4943-89f2-120020007030",
   "metadata": {},
   "source": [
    "We can see that none of these straight lines can be used as decision boundary nor any other lines going through the origin.\n",
    "We need a line where the intercept $b$ is not equal to 0. The solution consists in the addition of a bias term. \n",
    "\n",
    "For example the line $y = -x + 1.2$ could be used as a separating line for our problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e3677-e9ce-4961-b0c7-361176220767",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"r\")\n",
    "ax.scatter(1, 0, color=\"r\")\n",
    "ax.scatter(1, 1, color=\"g\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "m, c = -1, 1.2\n",
    "ax.plot(X, m * X + c )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc0a24-db46-416d-ae34-c0cc1ab783fc",
   "metadata": {},
   "source": [
    "The data points in above plot are linearly separable, and a single layer perceptron can do the job. \n",
    "\n",
    "However, what if your data looks like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41be7c-735c-4fcd-b6d9-6c119dcbe184",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"g\")\n",
    "ax.scatter(1, 0, color=\"g\")\n",
    "ax.scatter(1, 1, color=\"r\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "m = -1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54503b26-8394-4089-8938-70a8515e2e1d",
   "metadata": {},
   "source": [
    "This problem cannot be solved with a single layer perceptron. No matter which straight line you choose, you will never succeed in having the red points on one side and the green points on the other side. So there is no way for a single straight line separating those points. In this case, the data is not linearly separarable.\n",
    "\n",
    "To solve this problem, we need to introduce a 'network' of perceptrons with hidden layers.\n",
    "\n",
    "As we have shown before, one perceptron was enough to separate classes that are linearly separable. However, there are many clusters of classes, for which it will not work. We are going to have a look at some examples and will discuss cases where it will and will not be possible to separate the classes. Specifically, we are going to look at how to use a single layer perceptron to implement logical 'NOT', 'AND', 'OR' functions, and how to combine them together to implement the logical 'XOR' function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7ec79-ae06-4d04-bdb5-7636d3005f44",
   "metadata": {},
   "source": [
    "# Perceptron for the NOT Function\n",
    "\n",
    "In this example, we will program a perceptron which implements the 'NOT' function. It is defined in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee5b76-7a29-4da9-adc4-c5795951374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "table = [(0, 1), (1, 0)]\n",
    "headers = [\"A\", \"NOT A\"]\n",
    "print(tabulate(table, headers=headers, tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1c411-6b39-4d10-9b6e-72155c51065f",
   "metadata": {},
   "source": [
    "NOT($x$) is a 1-variable function, it means that we will have one input at a time. Also, it is a logical function, and so both the input and the output have only two possible states: 0 and 1 (i.e., False and True): the step function seems to fit our case since it produces a binary output.\n",
    "Given parameters, $w$ and $b$, it will perform the following computation: $\\hat{y} = f(wx + b)$.\n",
    "\n",
    "The fundamental question is: are there two values that, if picked as parameters, **allow the perceptron to implement the NOT logical function?** When saying that a perceptron implements a function, it means that for each input in the function’s domain the perceptron returns the same number (or vector) as the function would return for the same input.\n",
    "\n",
    "Back to our question: those values exist since we can easily find them: let’s pick $w = -1$ and $b = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dd4ef-3303-431d-9954-792c0abf6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unit_step(v):\n",
    "    \"\"\" Step function. v must be a scalar \"\"\"\n",
    "    if v >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def perceptron(x, w, b):\n",
    "    \"\"\" Function implemented by a perceptron with \n",
    "        weight vector w and bias b \"\"\"\n",
    "    v = np.dot(w, x) + b\n",
    "    y = unit_step(v)\n",
    "    return y\n",
    "\n",
    "def NOT_percep(x):\n",
    "    return perceptron(x, w=-1, b=0.5)\n",
    "\n",
    "print(\"NOT(0) = {}\".format(NOT_percep(0)))\n",
    "print(\"NOT(1) = {}\".format(NOT_percep(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dec5ca-001b-4137-bd80-c5b3cf345f88",
   "metadata": {},
   "source": [
    "We conclude that the answer to the initial question is: yes, a perceptron can implement the NOT logical function. We just need to properly set its parameters. Notice that the above solution isn’t unique; in fact, solutions, intended as $(w, b)$ points, are infinite for this particular problem! You can use your favorite one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d268b7-fdab-48d5-9299-8f7e01bffab3",
   "metadata": {},
   "source": [
    "# Perceptron for the AND Function\n",
    "\n",
    "The next question is: **Can a perceptron implement the AND logical function?**\n",
    "\n",
    "The AND logical function is a 2-variables function, AND($x_1, x_2$), with binary inputs and output. Below is the truth table for logical AND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee069817-25be-486a-a3ce-75888d0c497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [(0, 0, 0),\n",
    "         (0, 1, 0),\n",
    "         (1, 0, 0),\n",
    "         (1, 1, 1)]\n",
    "\n",
    "headers = [\"A\", \"B\", \"A AND B\"]\n",
    "print(tabulate(table, headers=headers, tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2686ab-3e76-437c-8e05-2a49161646fc",
   "metadata": {},
   "source": [
    "The perceptron we are going to use is shown in the following:\n",
    "\n",
    "$$\\hat{y} = f(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "This time, we have three parameters: $w_1$, $w_2$ and $b$.\n",
    "Can you guess which are three values for these parameters which would allow the perceptron to solve the AND problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e903fb-d529-4f5b-bfe6-83f6d9276466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND_percep(x):\n",
    "    w = np.array([1, 1])\n",
    "    b = -1.5\n",
    "    return perceptron(x, w, b)\n",
    "\n",
    "# Test\n",
    "example1 = np.array([1, 1])\n",
    "example2 = np.array([1, 0])\n",
    "example3 = np.array([0, 1])\n",
    "example4 = np.array([0, 0])\n",
    "\n",
    "print(\"AND({}, {}) = {}\".format(1, 1, AND_percep(example1)))\n",
    "print(\"AND({}, {}) = {}\".format(1, 0, AND_percep(example2)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 1, AND_percep(example3)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 0, AND_percep(example4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cccdbfa-dafb-40e0-bb46-92c069da0781",
   "metadata": {},
   "source": [
    "One possible solution for logical AND: $w_1 = 1, w_2 = 1, b = -1.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a5da7-663c-4121-b512-4a0c8a283465",
   "metadata": {},
   "source": [
    "# Perceptron for the OR Function\n",
    "OR($x_1, x_2$) is a 2-variables function too, and its output is 1-dimensional (i.e., one number) and has two possible states (0 or 1). Therefore, we will use a perceptron with the same architecture as the one before. **Which are the three parameters that solve the OR problem?** Below is the truth table for logical OR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df5f30-7908-4973-8f81-1dd2b48e91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_table = [\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1]\n",
    "]\n",
    "headers = [\"A\", \"B\", \"A OR B\"]\n",
    "table = tabulate(truth_table, headers, tablefmt=\"pipe\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a2ec3-182c-4345-81cb-1ebdf3691b43",
   "metadata": {},
   "source": [
    "Here shows the code that implements the OR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea681f-88c1-48a4-915d-2bc8e39d71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_percep(x):\n",
    "    w = np.array([1, 1])\n",
    "    b = -0.5\n",
    "    return perceptron(x, w, b)\n",
    "\n",
    "# Test\n",
    "example1 = np.array([1, 1])\n",
    "example2 = np.array([1, 0])\n",
    "example3 = np.array([0, 1])\n",
    "example4 = np.array([0, 0])\n",
    "\n",
    "print(\"OR({}, {}) = {}\".format(1, 1, OR_percep(example1)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 0, OR_percep(example2)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 1, OR_percep(example3)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 0, OR_percep(example4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7036e-3985-4bc9-b6a4-f0df2c131ef5",
   "metadata": {},
   "source": [
    "One possible solution for logical OR: $w_1 = 1, w_2 = 1, b = -0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1f31b-acce-4fd6-be10-c5ddc15507bb",
   "metadata": {},
   "source": [
    "# XOR Function\n",
    "We conclude that a single perceptron with a step activation function can implement each one of the fundamental logical functions: NOT, AND and OR.\n",
    "They are called fundamental because any logical function, no matter how complex, can be obtained by a combination of those three. We can infer that, if we appropriately connect the three perceptrons we just built, we can implement any logical function.\n",
    "\n",
    "**How can we build a network of fundamental logical perceptrons so that it implements the XOR function?** Below is the truth table for logical XOR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d41dd-ee4e-4ed7-b0b3-12d4ac48c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_table = [\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0]\n",
    "]\n",
    "\n",
    "headers = [\"A\", \"B\", \"A XOR B\"]\n",
    "table = tabulate(truth_table, headers, tablefmt=\"pipe\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ddb44d-9822-4a77-858a-b19a3c403ca1",
   "metadata": {},
   "source": [
    "Here shows the code that implements the XOR function by combining the single layer perceptrons we just built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459db33-a13c-41d8-967c-b9db2ab7838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_net(x):\n",
    "    gate_1 = AND_percep(x)\n",
    "    gate_2 = NOT_percep(gate_1)\n",
    "    gate_3 = OR_percep(x)\n",
    "    new_x = np.array([gate_2, gate_3])\n",
    "    output = AND_percep(new_x)\n",
    "    return output\n",
    "\n",
    "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_net(example1)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_net(example2)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_net(example3)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_net(example4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a7011-4a1f-40dd-a2b8-4b33c3b3424c",
   "metadata": {},
   "source": [
    "These are the predictions we were looking for. We just combined the three perceptrons above to get a more complex logical function.\n",
    "\n",
    "You may be wondering if, as we did for the previous functions, it is possible to find parameters’ values for a single perceptron so that it solves the XOR problem all by itself. The answer is that they do not exist. Because the XOR problem is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86d030-1c48-4aca-8913-b515c91b6101",
   "metadata": {},
   "source": [
    "---\n",
    "***end***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
